name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Lint with Ruff
        run: ruff check .

      - name: Check formatting with Ruff
        run: ruff format --check .

      - name: Type check with mypy
        run: mypy chronicle_mcp/

      - name: Run pytest with coverage
        run: pytest --cov=chronicle_mcp --cov-report=xml --cov-report=term-missing

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install pre-commit
        run: pip install pre-commit

      - name: Install pre-commit hooks
        run: pre-commit install-hooks

      - name: Run pre-commit
        run: pre-commit run --all-files

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run safety check
        run: |
          pip install safety
          safety scan || echo "Skipping safety check (requires authentication)"

      - name: Run dependency check
        run: |
          pip install deptry
          deptry .

      - name: Check for known vulnerabilities
        run: pip-audit --requirement <(pip freeze) --ignore-vuln CVE-2025-69872

  codespell:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run codespell
        uses: codespell-project/actions-codespell@v2
        with:
          skip: ".git,*.svg,*.png,*.md,.github/workflows/ci.yml"
          ignore_words_list: "and,param,be,to,the"

  cross-platform-test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.11"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run tests
        run: pytest tests/ -v

  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run benchmark tests
        run: |
          # Run benchmarks and capture output
          pytest tests/ -v --benchmark-only --benchmark-json=benchmark.json || true

          # Check if benchmark file exists and has valid content
          if [ -f benchmark.json ] && [ -s benchmark.json ]; then
            # Validate JSON is not empty
            if python -c "import json; json.load(open('benchmark.json'))" 2>/dev/null; then
              echo "Benchmark results found"
            else
              echo "No benchmark tests found - removing empty file"
              rm -f benchmark.json
            fi
          else
            echo "No benchmark tests found"
          fi

      - name: Upload benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        if: hashFiles('benchmark.json') != ''
        with:
          name: Python Benchmark
          tool: pytest
          output-file-path: benchmark.json
          save-data-file: true
          comment-on-alert: false
          summary-always: false
